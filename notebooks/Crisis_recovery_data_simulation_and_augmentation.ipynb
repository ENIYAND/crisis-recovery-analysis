{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1333107a-8288-4332-b221-1b0f1339092b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Simulation and Augmentation\n",
    "\n",
    "## Notebook Purpose\n",
    "\n",
    "\n",
    "Real production systems rarely provide clean, crisis-ready datasets. To demonstrate end-to-end data engineering, analytics, and ML decisioning skills, we must first *simulate* a realistic operating environment.\n",
    "\n",
    "This notebook is responsible for **creating a high-fidelity synthetic dataset** that mimics a real food-delivery platform (QuickBite) during a crisis period.\n",
    "\n",
    "This notebook deliberately runs in **batch / offline mode** and does **NOT** use streaming or Auto Loader. Its sole responsibility is **data generation and augmentation**, not ingestion.\n",
    "\n",
    "---\n",
    "\n",
    "## Business Context\n",
    "\n",
    "QuickBite experienced a **monsoon-driven operational crisis** causing:\n",
    "- Late deliveries\n",
    "- Food safety complaints\n",
    "- Increased customer churn risk\n",
    "\n",
    "However, historical public datasets do not contain:\n",
    "- Crisis signals\n",
    "- Customer sentiment\n",
    "- Operational stress indicators\n",
    "\n",
    "Therefore, I simulate:\n",
    "- A time-shifted dataset (2025)\n",
    "- Crisis impact windows\n",
    "- Customer behavior changes\n",
    "- Review sentiment signals\n",
    "\n",
    "These outputs will later be treated **as if they were generated by a live application**.\n",
    "________________________________________\n",
    "## Outputs of This Notebook\n",
    "\n",
    "This notebook produces **foundational tables** used downstream:\n",
    "\n",
    "| Table | Purpose |\n",
    "|------|--------|\n",
    "| `fact_orders` | Simulated order-level facts (ground truth) |\n",
    "| `fact_reviews` | Customer reviews + sentiment labels |\n",
    "| `dim_customers` | Synthetic customer profiles |\n",
    "\n",
    "---\n",
    "\n",
    "Note:\n",
    "_ML-ready features (ml_churn_features) are intentionally created in a\n",
    "separate notebook (Crisis_recovery_ml_feature_engineering) to ensure\n",
    "clean separation between data generation and feature engineering._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccba23c1-a480-4205-9e12-19dbeb265725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kaggle\n",
    "%pip install faker\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91748392-6d4d-4edd-8170-232db24f08f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1: Importing the Public Dataset\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "We need a **realistic base distribution** of orders to avoid artificial patterns.\n",
    "\n",
    "### Approach\n",
    "\n",
    "We ingest a public DoorDash-like dataset and treat it as **historical operational truth**.\n",
    "\n",
    "### Key Decisions\n",
    "\n",
    "- CSV chosen for transparency\n",
    "- Schema inference enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed9cee8-edc6-4337-8012-edb81233214b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# # Paste the exact path you copied here\n",
    "kaggle_json_path = \"/Volumes/workspace/default/kaggle_config/kaggle.json\"\n",
    "\n",
    "# # Kaggle expects the directory, not the file itself\n",
    "os.environ[\"KAGGLE_CONFIG_DIR\"] = os.path.dirname(kaggle_json_path)\n",
    "\n",
    "src = \"/Volumes/workspace/default/kaggle_config/kaggle.json\"\n",
    "dst_dir = \"/tmp/.kaggle\"\n",
    "dst = f\"{dst_dir}/kaggle.json\"\n",
    "\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "shutil.copy(src, dst)\n",
    "\n",
    "print(\"Copied kaggle.json to:\", dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3223ef36-7251-49fd-85e7-8b21b1cc93bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/tmp/.kaggle\"\n",
    "print(\"KAGGLE_CONFIG_DIR set to:\", os.environ[\"KAGGLE_CONFIG_DIR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cdc7b73-6663-4ff6-a8b6-6f765582a7a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "mkdir -p /tmp/doordash_data\n",
    "\n",
    "kaggle datasets download \\\n",
    "  -d dharun4772/doordash-eta-prediction \\\n",
    "  -p /tmp/doordash_data \\\n",
    "  --unzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8807bce6-225d-442c-9f21-c37fcd074609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "cd /tmp/doordash_data\n",
    "unzip -o doordash-eta-prediction.zip\n",
    "ls -lh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a55e692-4f3b-43c0-8605-f1b9d5af974c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "head -5 /tmp/doordash_data/*.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e74644a1-eb78-4baa-9813-3528333e55ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### food_delivery SCHEMA and VOLUME creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b40b043-8391-4e92-aeed-c988eacee216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS workspace.food_delivery\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c72a901b-c58c-4860-80e8-aad38843219c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS workspace.food_delivery.food_delivery_data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1e0cbba-73f5-4d32-9d8e-a04fb5a5d086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "mv /tmp/doordash_data/*.csv /Volumes/workspace/food_delivery/food_delivery_data/\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e45ad7a1-b834-4178-8693-b7f4a88c1c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2: Time Warp Simulation (Temporal Shift)\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "The dataset ends years before our crisis scenario. Dashboards and ML models must reflect **current timelines**.\n",
    "\n",
    "### Approach\n",
    "\n",
    "- Identify max historical timestamp\n",
    "- Warp all events forward so the dataset ends in **Dec 2025**\n",
    "\n",
    "### Why this approach\n",
    "\n",
    "- Preserves **relative temporal patterns**\n",
    "- Avoids fabricating random timestamps\n",
    "- Enables realistic seasonality analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 3: Order ID & Deterministic Identity\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "The public dataset lacks globally unique order identifiers suitable for joins.\n",
    "\n",
    "### Approach\n",
    "\n",
    "- Generate monotonic `order_id`\n",
    "- Ensure deterministic joins across tables\n",
    "\n",
    "### Design Choice\n",
    "\n",
    "We intentionally **do not reuse existing IDs** to prevent accidental leakage of source semantics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bf51268-ea6f-4e62-bf5f-d9d5afb031cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load the data, treating \"NA\" as proper NULLs\n",
    "volume_path = \"/Volumes/workspace/food_delivery/food_delivery_data/historical_data.csv\"\n",
    "\n",
    "df_backbone = (spark.read.format(\"csv\") \n",
    "  .option(\"header\", \"true\") \n",
    "  .option(\"inferSchema\", \"true\") \n",
    "  .option(\"nullValue\", \"NA\") \n",
    "  .load(volume_path))\n",
    "\n",
    "# 3. Check the date range to determine our \"Warp Factor\"\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "print(\"Data Schema:\")\n",
    "df_backbone.printSchema()\n",
    "\n",
    "print(\"\\nDate Range:\")\n",
    "df_backbone.select(min(\"created_at\"), max(\"created_at\")).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4362f5c-c792-40ec-9287-87a1ea744be6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, count\n",
    "\n",
    "# Create a deterministic ordering of all rows\n",
    "# This ensures timestamps progress consistently\n",
    "w = Window.orderBy(\"created_at\")\n",
    "\n",
    "df_indexed = df_backbone.withColumn(\n",
    "    \"row_num\",\n",
    "    row_number().over(w)\n",
    ")\n",
    "\n",
    "# Total number of rows (used for linear interpolation)\n",
    "total_rows = df_indexed.count()\n",
    "\n",
    "# Define the simulated crisis window\n",
    "target_start = \"2025-07-01 00:00:00\"\n",
    "target_end   = \"2025-12-31 23:59:59\"\n",
    "\n",
    "# Generate simulated timestamps by spreading rows evenly across the window\n",
    "df_simulated = (\n",
    "    df_indexed\n",
    "    .withColumn(\n",
    "        \"created_at_simulated\",\n",
    "        expr(f\"\"\"\n",
    "            CAST(\n",
    "              from_unixtime(\n",
    "                unix_timestamp('{target_start}')\n",
    "                + (\n",
    "                    (row_num - 1) *\n",
    "                    (\n",
    "                        (unix_timestamp('{target_end}')\n",
    "                        - unix_timestamp('{target_start}'))\n",
    "                        / ({total_rows} - 1)\n",
    "                    )\n",
    "                )\n",
    "              ) AS TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "    )\n",
    "    # Simulate actual delivery time using estimated driving duration\n",
    "    .withColumn(\n",
    "        \"actual_delivery_time_simulated\",\n",
    "        expr(\"\"\"\n",
    "            CAST(\n",
    "              created_at_simulated\n",
    "              + estimated_store_to_consumer_driving_duration * INTERVAL 1 SECOND\n",
    "              AS TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "    )\n",
    "    # Generate a unique surrogate order_id\n",
    "    .withColumn(\"order_id\", monotonically_increasing_id())\n",
    "    .drop(\"created_at\", \"actual_delivery_time\", \"row_num\")\n",
    ")\n",
    "\n",
    "\n",
    "df_simulated = df_simulated.select( \"order_id\", \"market_id\", \"store_id\", \"store_primary_category\", \"order_protocol\", \"total_items\", \"subtotal\", \"num_distinct_items\", \"min_item_price\", \"max_item_price\", \"total_onshift_dashers\", \"total_busy_dashers\", \"total_outstanding_orders\", \"estimated_order_place_duration\", \"estimated_store_to_consumer_driving_duration\", \"created_at_simulated\", \"actual_delivery_time_simulated\" )\n",
    "\n",
    "print(\"Data Schema:\")\n",
    "df_simulated.printSchema()\n",
    "df_simulated.show(5)\n",
    "\n",
    "\n",
    "# 5. Verify the new range\n",
    "df_simulated.select(min(\"created_at_simulated\"), max(\"created_at_simulated\")).show()\n",
    "\n",
    "display(df_simulated.select(\n",
    "    \"order_id\", \n",
    "    \"total_items\",\n",
    "    \"order_protocol\",\n",
    "    \"market_id\",\n",
    "    \"store_id\",\n",
    "    \"created_at_simulated\",\n",
    "\n",
    ").limit(5))\n",
    "\n",
    "print(\"Data loaded and ID generated.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a0a84a4-c648-4d70-89b9-d1e43d70a24f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, count\n",
    "\n",
    "df_simulated \\\n",
    "    .groupBy(\n",
    "        year(\"created_at_simulated\").alias(\"year\"),\n",
    "        month(\"created_at_simulated\").alias(\"month\")\n",
    "    ) \\\n",
    "    .agg(count(\"*\").alias(\"rows\")) \\\n",
    "    .orderBy(\"year\", \"month\") \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "359b58f6-7b1c-48d2-88eb-c322611e9b53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4: Customer Simulation\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Customer-level analytics and churn modeling require:\n",
    "- Stable identities\n",
    "- Behavioral diversity\n",
    "\n",
    "### Approach\n",
    "\n",
    "- Randomly assign orders to ~5,000 customers\n",
    "- Create a separate customer dimension table\n",
    "\n",
    "### Why not real customer IDs?\n",
    "\n",
    "- Public data lacks PII\n",
    "- Synthetic profiles allow controlled experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41398a25-d402-4310-a9e6-b390ad73cf72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Assign Synthetic Customer IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c72bf141-db8c-4843-a34e-85744ace65bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, floor\n",
    "\n",
    "# Number of synthetic customers to simulate\n",
    "num_customers = 5000\n",
    "\n",
    "# Idempotent creation of fact_orders\n",
    "if not spark.catalog.tableExists(\"food_delivery.fact_orders\"):\n",
    "    print(\"Creating fact_orders with frozen random customer_ids\")\n",
    "\n",
    "    # Randomly assign each order to a customer\n",
    "    df_orders = (\n",
    "        df_simulated\n",
    "        .withColumn(\"customer_id\", floor(rand() * num_customers) + 1)\n",
    "    )\n",
    "\n",
    "    # Persist the result as a Delta table\n",
    "    df_orders.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"food_delivery.fact_orders\")\n",
    "\n",
    "else:\n",
    "    print(\"fact_orders already exists — reusing it\")\n",
    "\n",
    "    df_orders = spark.table(\"food_delivery.fact_orders\")\n",
    "\n",
    "df_orders.printSchema()\n",
    "\n",
    "display(df_orders.select(\n",
    "    \"order_id\",\n",
    "    \"customer_id\",\n",
    "    \"total_items\",\n",
    "    \"order_protocol\",\n",
    "    \"market_id\",\n",
    "    \"store_id\",\n",
    "    \"created_at_simulated\"\n",
    "\n",
    ").limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d76b374-999e-46da-a13a-50a758d3c2b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate Customer Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a473e51-7d02-4ae7-afb5-57c9835edbe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Define our table name explicitly\n",
    "table_name = \"food_delivery.dim_customers\"\n",
    "\n",
    "# 1. Check if the table already exists\n",
    "if not spark.catalog.tableExists(table_name):\n",
    "    print(f\"Generating {table_name} from scratch...\")\n",
    "\n",
    "    fake = Faker()\n",
    "    customer_data = []\n",
    "    segments = [\"Student\", \"Young Professional\", \"Family\", \"Corporate\"]\n",
    "\n",
    "    # Use the num_customers variable we defined earlier\n",
    "    for i in range(1, num_customers + 1):\n",
    "        profile = {\n",
    "            \"customer_id\": i,\n",
    "            \"customer_name\": fake.name(),\n",
    "            \"segment\": random.choice(segments),\n",
    "            #need to cast the date to string or timestamp for Spark compatibility\n",
    "            \"signup_date\": str(fake.date_between(start_date='-2y', end_date='today')),\n",
    "            \"email\": fake.email()\n",
    "        }\n",
    "        customer_data.append(profile)\n",
    "\n",
    "    # Convert to Spark DataFrame\n",
    "    df_customers = spark.createDataFrame(pd.DataFrame(customer_data))\n",
    "\n",
    "    # Write as Delta\n",
    "    df_customers.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    print(\"Customer profiles generated and frozen.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Table {table_name} already exists. Loading frozen data...\")\n",
    "    df_customers = spark.table(table_name)\n",
    "\n",
    "# Display to verify\n",
    "display(df_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be9a64ef-2598-4368-acdb-73f83086f330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join Orders with Customers\n",
    "df_enriched = df_orders.join(df_customers, \"customer_id\", \"left\")\n",
    "\n",
    "# Select a few key columns to inspect\n",
    "display(df_enriched.select(\n",
    "    \"order_id\", #\n",
    "    \"customer_name\",\n",
    "    \"segment\",\n",
    "    \"created_at_simulated\",\n",
    "    \"actual_delivery_time_simulated\"\n",
    ").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d699547-12c4-418c-8feb-be24e00ce7e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5: Crisis Injection Logic\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "To test recovery systems, we need a **controlled crisis window**.\n",
    "\n",
    "### Approach\n",
    "\n",
    "- Identify Oct 2025 as crisis window\n",
    "- Inflate delivery times\n",
    "- Increase negative review probability\n",
    "\n",
    "### Why deterministic windows?\n",
    "\n",
    "- Enables before/after comparisons\n",
    "- Simplifies evaluation for dashboards and ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78707ad1-06b0-4f98-a373-9a1d0b62aa00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, rand\n",
    "\n",
    "# 1. Load the frozen orders\n",
    "df_orders = spark.table(\"food_delivery.fact_orders\")\n",
    "\n",
    "# 2. Define the Crisis Period (October 2025)\n",
    "# I will target orders created in October 2025\n",
    "crisis_condition = (col(\"created_at_simulated\") >= \"2025-10-01\") & \\\n",
    "                   (col(\"created_at_simulated\") <= \"2025-10-30\")\n",
    "\n",
    "if not spark.catalog.tableExists(\"food_delivery.fact_orders\"):\n",
    "\n",
    "    # 3. Inject the \"Monsoon Delay\"\n",
    "    # Logic: If it's October 2025, multiply delivery time by 1.5x to 2.0x (Random chaos)\n",
    "    df_orders_crisis = df_orders.withColumn(\n",
    "        \"actual_delivery_time_simulated\",\n",
    "        when(\n",
    "            crisis_condition,\n",
    "            col(\"actual_delivery_time_simulated\")\n",
    "            + expr(\"INTERVAL 2700 SECONDS\")                       # base 45 min\n",
    "            + expr(\"CAST(rand() * 2700 AS INT) * INTERVAL 1 SECONDS\")  # +0–45 min\n",
    "        ).otherwise(col(\"actual_delivery_time_simulated\"))\n",
    "    ).withColumn(\n",
    "        \"created_at_simulated\",\n",
    "        col(\"created_at_simulated\").cast(\"timestamp\")\n",
    "    ) #To keep created_at_simulated also in timestamp\n",
    "\n",
    "    # 4. Overwrite the table with this new \"Broken\" data\n",
    "    df_orders_crisis.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"food_delivery.fact_orders\")\n",
    "\n",
    "    print(\"Crisis injected: Oct 2025 delivery times have been destabilized.\")\n",
    "else:\n",
    "    print(\"Crisis already injected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb3d78fc-0ce3-4540-9825-e690222bf904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"food_delivery.fact_orders\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45adf3bf-f173-46a9-b0cd-131d01f92c86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6: Review & Sentiment Simulation\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Crisis detection requires **qualitative signals**, not just numbers.\n",
    "\n",
    "### Approach\n",
    "\n",
    "We simulate customer reviews based on:\n",
    "- Delivery delays\n",
    "- Random food safety incidents\n",
    "\n",
    "Sentiment categories:\n",
    "- Positive\n",
    "- Late Delivery\n",
    "- Food Safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a5ae4cc-d83b-4d0d-a772-f5f85a542cc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# 1. Load the (now delayed) orders\n",
    "df_orders = spark.table(\"food_delivery.fact_orders\")\n",
    "\n",
    "# 2. Simulate an \"Estimated Delivery Time\" (Target)\n",
    "# I assume the 'Target' was 45 mins (2700s) after creation. \n",
    "# In reality, this varies, but this baseline works for calculating \"Lateness\".\n",
    "df_orders_w_delay = df_orders.withColumn(\n",
    "    \"delay_seconds\", \n",
    "    (F.col(\"actual_delivery_time_simulated\").cast(\"long\") - F.col(\"created_at_simulated\").cast(\"long\")) - 2700\n",
    ")\n",
    "\n",
    "# 3. Define the Review Logic (The \"Brain\" of the customer)\n",
    "def get_review_sentiment(delay_sec, is_food_safety_incident):\n",
    "    # Case A: The Food Safety Scandal (Random 5% trigger passed in)\n",
    "    if is_food_safety_incident:\n",
    "        return \"Critical\", 1, \"Food poisoning risk! Undercooked and smelled bad. Never ordering again.\"\n",
    "    \n",
    "    # Case B: The Monsoon/Late Delivery (> 45 mins late)\n",
    "    elif delay_sec > 2700: \n",
    "        return \"Negative\", 2, \"Took forever to arrive. Food was cold due to delay.\"\n",
    "    \n",
    "    # Case C: Happy Path\n",
    "    else:\n",
    "        return \"Positive\", 5, \"Great service, arrived on time!\"\n",
    "\n",
    "# I have written UDF as an alter approach (User Defined Function) but it isn't efficient for large data, \n",
    "# so i will use Spark SQL Native functions for speed.\n",
    "\n",
    "# 4. Generate the Reviews using Spark Logic\n",
    "df_reviews = df_orders_w_delay.withColumn(\"is_safety_incident\", F.rand() < 0.05) \\\n",
    "    .withColumn(\n",
    "        \"review_score\",\n",
    "        F.when(F.col(\"is_safety_incident\"), 1)\n",
    "         .when(F.col(\"delay_seconds\") > 2700, 2)\n",
    "         .otherwise(5)\n",
    "    ).withColumn(\n",
    "        \"review_text\",\n",
    "        F.when(F.col(\"is_safety_incident\"), \"Food tasted off and undercooked. Felt sick afterwards. #HealthHazard\")\n",
    "         .when(F.col(\"delay_seconds\") > 2700, \"Delivery took way too long. Food was completely cold.\")\n",
    "         .otherwise(\"Delicious and on time! Loved it.\")\n",
    "    ).withColumn(\n",
    "        \"sentiment_category\",\n",
    "        F.when(F.col(\"is_safety_incident\"), \"Food Safety\")\n",
    "         .when(F.col(\"delay_seconds\") > 2700, \"Late Delivery\")\n",
    "         .otherwise(\"Positive\")\n",
    "    )\n",
    "\n",
    "# 5. Select only relevant columns for the Reviews Table\n",
    "df_fact_reviews = df_reviews.select(\n",
    "    \"order_id\",\n",
    "    \"customer_id\",\n",
    "    \"review_score\",\n",
    "    \"review_text\",\n",
    "    \"sentiment_category\"\n",
    ")\n",
    "\n",
    "\n",
    "table_name = \"food_delivery.fact_reviews\"\n",
    "if not spark.catalog.tableExists(table_name):\n",
    "    print(f\"Generating {table_name}...\")\n",
    "    df_fact_reviews.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    print(\"Reviews generated and frozen.\")\n",
    "else:\n",
    "    print(f\"{table_name} already exists. Skipping generation.\")\n",
    "\n",
    "display(spark.table(table_name).limit(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1167b6dd-f947-4bf6-a395-5b0c0a239308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Checking the crisis spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd61baf5-512d-496e-ae00-a74ede2d6b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT \n",
    "  DATE_FORMAT(o.created_at_simulated, 'yyyy-MM') as month_year,\n",
    "  r.sentiment_category,\n",
    "  COUNT(*) as review_count\n",
    "FROM food_delivery.fact_orders o\n",
    "JOIN food_delivery.fact_reviews r \n",
    "  ON o.order_id = r.order_id\n",
    "WHERE o.created_at_simulated BETWEEN '2025-07-01' AND '2025-12-31'\n",
    "GROUP BY 1, 2\n",
    "ORDER BY 1, 2;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c75eeba-223c-4610-8b0b-54a38346f7e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "October 2025 Spike: Even though Food safety complaints is distributed evenly across the months ,we have 29,550 \"Late Delivery\" complaints on October. compared to 0 in September, this is exactly the kind of \"anomaly\" which was intended to capture."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Crisis_recovery_data_simulation_and_augmentation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
