{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48f55d9a-549b-440d-8500-7885e4450e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Layer – Raw Order Ingestion (Auto Loader)\n",
    "\n",
    "## Purpose\n",
    "This notebook builds the **Bronze layer** of the Crisis Recovery Analytics platform. The Bronze layer represents the *raw, immutable system of record* and is responsible for ingesting application-level order events with minimal transformation.\n",
    "\n",
    "The design follows real-world Databricks Lakehouse best practices using **Auto Loader**, enabling scalable, fault-tolerant, and idempotent ingestion.\n",
    "\n",
    "---\n",
    "\n",
    "## Business Context\n",
    "During a simulated crisis recovery period for a food delivery platform, large volumes of order events arrive continuously from operational systems. These events may:\n",
    "- Arrive late or out of order\n",
    "- Contain schema drift (newx fields)\n",
    "- Include partially corrupt records\n",
    "\n",
    "The Bronze layer ensures **no data loss** while preserving raw fidelity for downstream analytics.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "- JSON files written to the Landing Zone\n",
    "  - Path: `/Volumes/workspace/food_delivery/landing_zone/orders`\n",
    "  - Represents raw application logs (simulated)\n",
    "\n",
    "## Outputs\n",
    "- Delta table: `food_delivery.bronze_orders`\n",
    "- Schema evolution metadata\n",
    "- Streaming checkpoint state\n",
    "\n",
    "---\n",
    "\n",
    "## Key Design Decisions\n",
    "\n",
    "### Why Auto Loader?\n",
    "- Tracks processed files via checkpoints\n",
    "- Prevents duplicate ingestion on notebook re-runs\n",
    "- Supports schema evolution without pipeline failure\n",
    "\n",
    "### Why JSON + Landing Zone?\n",
    "- Mimics real-world S3 / ADLS ingestion patterns\n",
    "- Decouples producers (apps) from consumers (analytics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2742f14-95a5-4e27-ad2e-22e2ae34f7a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1: Landing Zone Validation\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "In production environments, notebooks may be:\n",
    "- Re-run manually\n",
    "- Re-triggered by schedulers\n",
    "- Restarted after failures\n",
    "\n",
    "If raw files are regenerated on every run, it can lead to:\n",
    "- Duplicate ingestion\n",
    "- Inconsistent downstream metrics\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "Before exporting any raw JSON files, we:\n",
    "- Check whether the landing zone already contains data\n",
    "- Skip regeneration if files exist\n",
    "\n",
    "This ensures **notebook-level idempotency**.\n",
    "\n",
    "---\n",
    "\n",
    "### Design Decision\n",
    "\n",
    "We prefer **idempotent checks** over manual cleanup because:\n",
    "- Production pipelines must be restart-safe\n",
    "- Engineers should not rely on human intervention\n",
    "- Replays should not corrupt data\n",
    "\n",
    "## 2: Simulating Application Raw Events\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Our upstream system (QuickBite app) does not exist.\n",
    "However, downstream ingestion pipelines expect:\n",
    "- Semi-structured JSON logs\n",
    "- Partitioned by time\n",
    "- Written incrementally\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We simulate the application by:\n",
    "- Reading from `fact_orders`\n",
    "- Writing JSON files into the landing zone\n",
    "- Partitioning by `year` and `month`\n",
    "\n",
    "This mirrors how:\n",
    "- Mobile apps\n",
    "- Backend services\n",
    "- Event pipelines\n",
    "\n",
    "write data into cloud object storage.\n",
    "\n",
    "---\n",
    "\n",
    "### Why JSON?\n",
    "\n",
    "- Schema-flexible\n",
    "- Common for event logs\n",
    "- Easy to evolve without breaking ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f435b11-5671-48ab-b0a3-5cd383253836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS workspace.food_delivery.landing_zone;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac045c4-44ff-417d-a4d4-c5ea38c2631b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Landing zone simulating application object storage\n",
    "landing_path = \"/Volumes/workspace/food_delivery/landing_zone/orders\"\n",
    "\n",
    "# Utility function to check whether landing zone already has files\n",
    "def landing_zone_is_empty(path):\n",
    "    try:\n",
    "        return len(dbutils.fs.ls(path)) == 0\n",
    "    except:\n",
    "        # Path does not exist yet\n",
    "        return True\n",
    "\n",
    "# Idempotency check: export raw files only once\n",
    "if landing_zone_is_empty(landing_path):\n",
    "    print(\"Landing zone empty → exporting raw JSON files\")\n",
    "\n",
    "    # Source data representing application ground truth\n",
    "    df_orders = spark.table(\"food_delivery.fact_orders\")\n",
    "\n",
    "    # Write raw events as JSON, partitioned by time\n",
    "    # This simulates how real apps write logs to cloud storage\n",
    "    (\n",
    "        df_orders\n",
    "        .withColumn(\"year\", F.year(\"created_at_simulated\"))\n",
    "        .withColumn(\"month\", F.month(\"created_at_simulated\"))\n",
    "        .write\n",
    "        .format(\"json\")\n",
    "        .mode(\"append\")              # Append is safe because export is guarded\n",
    "        .partitionBy(\"year\", \"month\")\n",
    "        .save(landing_path)\n",
    "    )\n",
    "\n",
    "    print(\"Raw JSON files written once.\")\n",
    "else:\n",
    "    print(\"Landing zone already has data → skipping export.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8230190f-7517-4aad-9d75-c385d476ca06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3: Bronze Ingestion Using Auto Loader\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Traditional batch ingestion:\n",
    "- Reprocesses files on every run\n",
    "- Cannot safely handle schema changes\n",
    "- Is brittle during failures\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We use **Databricks Auto Loader** with:\n",
    "- `cloudFiles` source\n",
    "- Streaming semantics\n",
    "- Checkpoint-based state tracking\n",
    "\n",
    "Even though data is finite, we run Auto Loader in  \n",
    "**`availableNow` mode** to simulate production ingestion.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Auto Loader Instead of Batch Reads?\n",
    "\n",
    "| Reason | Benefit |\n",
    "|------|--------|\n",
    "| File tracking | Prevents duplicate ingestion |\n",
    "| Checkpoints | Fault-tolerant restarts |\n",
    "| Schema evolution | Handles new columns safely |\n",
    "| Rescued data | Preserves corrupt records |\n",
    "\n",
    "This is the **industry standard ingestion pattern**.\n",
    "\n",
    "\n",
    "## 4: Schema Evolution & Rescued Data\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Upstream systems change over time:\n",
    "- New columns are added\n",
    "- Data types drift\n",
    "- Partial corruption occurs\n",
    "\n",
    "Failing the pipeline on such events would cause:\n",
    "- Downtime\n",
    "- Data loss\n",
    "- Broken dashboards\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We explicitly enable:\n",
    "- Schema evolution (`addNewColumns`)\n",
    "- Schema location tracking\n",
    "- `_rescued_data` capture\n",
    "\n",
    "This ensures:\n",
    "- No data is dropped\n",
    "- Unknown fields are preserved\n",
    "- Debugging remains possible\n",
    "\n",
    "## 5: Checkpoints and Exactly-Once Semantics\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Streaming ingestion without checkpoints can:\n",
    "- Reprocess files\n",
    "- Duplicate records\n",
    "- Corrupt aggregates\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We configure a dedicated checkpoint directory:\n",
    "- Tracks processed files\n",
    "- Stores streaming state\n",
    "- Enables safe recovery\n",
    "\n",
    "This guarantees **exactly-once ingestion** even if:\n",
    "- The notebook is re-run\n",
    "- The cluster restarts\n",
    "- A failure occurs mid-ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49390d8c-4f2a-4563-bdd4-8c289a06140b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Source path where raw application JSON events arrive\n",
    "source_path = \"/Volumes/workspace/food_delivery/landing_zone/orders\"\n",
    "\n",
    "# Checkpoint path to track processed files (exactly-once ingestion)\n",
    "checkpoint_path = \"/Volumes/workspace/food_delivery/food_delivery_data/checkpoints/bronze_orders\"\n",
    "\n",
    "# Schema location to persist inferred schema and support schema evolution\n",
    "schema_path = \"/Volumes/workspace/food_delivery/food_delivery_data/schemas/bronze_orders\"\n",
    "\n",
    "# Target Bronze table\n",
    "table_name = \"food_delivery.bronze_orders\"\n",
    "\n",
    "# Idempotency check: do not re-ingest if Bronze table already exists\n",
    "if not spark.catalog.tableExists(table_name):\n",
    "    print(\"Bronze table not found → running Auto Loader\")\n",
    "\n",
    "    # Auto Loader stream definition (cloudFiles = production-grade ingestion)\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")                         # Enable Auto Loader\n",
    "        .option(\"cloudFiles.format\", \"json\")          # Raw app events are JSON\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "        .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "        .load(source_path)\n",
    "\n",
    "        # Write as Delta with partitioning for query efficiency\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .partitionBy(\"year\", \"month\")\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "\n",
    "        # availableNow = process all current files once (batch-style streaming)\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(table_name)\n",
    "    )\n",
    "\n",
    "    # Wait until ingestion completes\n",
    "    query.awaitTermination()\n",
    "    print(\"Bronze table created successfully.\")\n",
    "else:\n",
    "    print(\"Bronze table already exists → skipping ingestion.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06cde396-7c6f-4abf-bd3a-aebaad781610",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769496627936}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"food_delivery.bronze_orders\").limit(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76dc98ad-71c5-48fd-8564-a27e8b239523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "SELECT COUNT(*) FROM food_delivery.fact_orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62315c05-4228-4876-b767-900c64cb603c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM food_delivery.bronze_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03f36f7b-90ad-4ba9-a504-9a9bfe3a97cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Downstream Dependencies\n",
    "\n",
    "The Bronze table feeds:\n",
    "- `silver_orders_clean` (data validation & typing)\n",
    "- `silver_orders_enriched` (customer & review joins)\n",
    "- ML feature pipelines\n",
    "- Operational dashboards\n",
    "\n",
    "Any error here propagates everywhere —  \n",
    "which is why **Bronze must be boring, stable, and safe**.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook establishes a **production-grade ingestion foundation** by:\n",
    "- Simulating real application logs\n",
    "- Using Auto Loader for reliability\n",
    "- Enforcing idempotency and fault tolerance\n",
    "- Preserving raw data fidelity\n",
    "\n",
    "It forms the **backbone** of the Crisis Recovery analytics platform."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5303823769054441,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Crisis_recovery_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
