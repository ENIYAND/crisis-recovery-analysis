{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4287022d-fd75-4a54-ab03-5369b24f1a4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook builds the **Machine Learning Feature Engineering layer** of the Crisis Recovery Lakehouse.\n",
    "\n",
    "The purpose of this layer is to convert **clean, trusted Silver-layer data**\n",
    "into **customer-level, model-ready features** that can be used for:\n",
    "\n",
    "- Churn prediction\n",
    "- Crisis impact modeling\n",
    "- Retention strategy optimization\n",
    "- Machine learning experimentation and governance\n",
    "\n",
    "Unlike Gold tables, this notebook does **not** serve dashboards or BI tools.\n",
    "\n",
    "Instead, it focuses on:\n",
    "- Predictive signal quality\n",
    "- Feature consistency\n",
    "- Reproducibility\n",
    "- Safe handoff into ML pipelines (MLflow)\n",
    "\n",
    "---\n",
    "\n",
    "## Business Context\n",
    "\n",
    "During a crisis, customer churn is rarely random.\n",
    "\n",
    "Customers disengage due to:\n",
    "- Repeated delivery delays\n",
    "- Food safety concerns\n",
    "- Accumulation of negative sentiment\n",
    "- Reduced engagement following bad experiences\n",
    "\n",
    "Leadership and retention teams need predictive answers to:\n",
    "- Which customers are at risk because of the crisis?\n",
    "- Which customers should receive proactive offers?\n",
    "- Which customers are most valuable to retain?\n",
    "\n",
    "This notebook transforms behavioral and sentiment data\n",
    "into **quantitative signals** that machine learning models can learn from.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs and Outputs\n",
    "\n",
    "### Inputs (from Silver Layer)\n",
    "\n",
    "| Source Table | Purpose |\n",
    "|-------------|---------|\n",
    "| `silver_orders_enriched` | Order history, sentiment, customer attributes |\n",
    "| `fact_reviews` | Review scores and customer feedback |\n",
    "\n",
    "---\n",
    "\n",
    "### Output (ML Feature Table)\n",
    "\n",
    "| Table | Business Purpose |\n",
    "|------|------------------|\n",
    "| `ml_churn_features` | Customer-level feature set for churn modeling |\n",
    "\n",
    "---\n",
    "\n",
    "## Design Principles of the ML Feature Layer\n",
    "\n",
    "- Features are **aggregated at the customer level**\n",
    "- Every feature has a **clear behavioral meaning**\n",
    "- No future information leakage\n",
    "- Features are numerical and model-ready\n",
    "- Transformations are deterministic and reproducible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71707003-dc4c-4530-8fc3-f5848219e813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Basic Behavioral Features\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Before predicting churn, we must understand **baseline customer behavior**.\n",
    "\n",
    "Key questions:\n",
    "- How active is the customer?\n",
    "- How frequently do they order?\n",
    "- How long have they been engaged with the platform?\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We aggregate order data at the customer level to compute:\n",
    "- Total lifetime orders\n",
    "- First order date\n",
    "- Last order date\n",
    "\n",
    "These features represent **long-term engagement strength**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1e3075f-f77d-442b-9e34-f49969987d70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, count, sum, when, lit, datediff, current_date, max as max_col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "# 1. Define the Analysis Date (The \"Today\" of the simulation)\n",
    "simulated_today = lit(\"2025-12-31\")\n",
    "\n",
    "# 2. Load the Silver Data\n",
    "df_silver = spark.table(\"food_delivery.silver_orders_enriched\")\n",
    "\n",
    "# 3. Create Customer Features\n",
    "# We aggregate at the Customer Level\n",
    "customer_features = df_silver.groupBy(\"customer_id\", \"segment\").agg(\n",
    "    count(\"order_id\").alias(\"total_orders\"),\n",
    "    avg(\"review_score\").alias(\"avg_star_rating\"),\n",
    "    \n",
    "    # Feature: Late Ratio (Percentage of orders that were late)\n",
    "    (sum(when(col(\"sentiment_category\") == \"Late Delivery\", 1).otherwise(0)) / count(\"order_id\")).alias(\"late_order_ratio\"),\n",
    "    \n",
    "    # Feature: Days Since Last Order (Recency)\n",
    "    datediff(simulated_today, max_col(\"created_at_simulated\")).alias(\"recency_days\")\n",
    ")\n",
    "\n",
    "# 4. Define the Target (Label)\n",
    "# Rule: If Recency > 30 days, they are Churned (1). Else Active (0).\n",
    "training_data = customer_features.withColumn(\n",
    "    \"label\", \n",
    "    when(col(\"recency_days\") > 30, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Handle Categorical Data (Segment)\n",
    "# Machine Learning models need numbers, not strings like \"Student\".\n",
    "# StringIndexer converts \"Student\" -> 0.0, \"Corporate\" -> 1.0\n",
    "indexer = StringIndexer(inputCol=\"segment\", outputCol=\"segment_index\")\n",
    "training_data_indexed = indexer.fit(training_data).transform(training_data)\n",
    "\n",
    "# 6. Save as a Feature Table\n",
    "training_data_indexed.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"food_delivery.ml_churn_features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd2721f-d693-4ee2-9e61-9af0d95c6436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Basic Behavioral Features** :\n",
    "- total_orders\n",
    "- avg_star_rating\n",
    "- late_order_ratio\n",
    "- recency_days\n",
    "- segment_index\n",
    "- label\n",
    "\n",
    "These features tells about the customer and \"how active they are?\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b59c93c-2f0d-4342-af95-c9810ba7f807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Advanced Temporal & Crisis Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "477d09b7-78ca-4a0c-a85b-d03018bbfd08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. crisis_flag_per_customer\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "During an operational crisis, not all customers are impacted equally.\n",
    "Some customers experience **severe service degradation**, while others are\n",
    "largely unaffected.\n",
    "\n",
    "For churn prediction and retention prioritization, we need a way to identify:\n",
    "- Which customers were exposed to crisis-level delays\n",
    "- Whether the customer experienced **any extreme failure**, not just average delays\n",
    "\n",
    "Event-level delay metrics are too granular for customer-level modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We derive a **binary crisis exposure signal** at the customer level by:\n",
    "\n",
    "1. Computing delivery delay beyond the expected duration\n",
    "2. Flagging orders with **extreme delays** (greater than 45 minutes)\n",
    "3. Aggregating per customer using a max operation\n",
    "\n",
    "The resulting feature, `crisis_exposure_index`, indicates whether a customer\n",
    "experienced **at least one crisis-level incident**.\n",
    "\n",
    "This creates a robust, noise-resistant indicator suitable for ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1248e81d-b430-46ea-81c5-4535fa8f94e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, max as max_col\n",
    "\n",
    "# Load cleaned orders data\n",
    "df_orders = spark.table(\"food_delivery.silver_orders_clean\")\n",
    "\n",
    "# Calculate crisis exposure index per customer\n",
    "crisis_flag_per_customer = (\n",
    "    df_orders\n",
    "    # Calculate delivery delay in seconds\n",
    "    .withColumn(\n",
    "        \"delivery_delay_seconds\",\n",
    "        col(\"actual_delivery_time_simulated\").cast(\"long\")\n",
    "        - col(\"created_at_simulated\").cast(\"long\")\n",
    "        - col(\"estimated_store_to_consumer_driving_duration\")\n",
    "    )\n",
    "    # Flag orders with delivery delay > 45 minutes (2700 seconds) as crisis\n",
    "    .withColumn(\n",
    "        \"crisis_flag\",\n",
    "        when(col(\"delivery_delay_seconds\") > 2700, 1).otherwise(0)\n",
    "    )\n",
    "    # Aggregate to get the maximum crisis flag per customer\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        max_col(\"crisis_flag\").alias(\"crisis_exposure_index\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "697078cd-703e-4639-a387-76b8a836a9a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. sentiment_velocity\n",
    "### Business Problem\n",
    "\n",
    "Customer churn is often triggered not by static dissatisfaction,\n",
    "but by **sudden deterioration in experience**.\n",
    "\n",
    "A customer whose sentiment is rapidly worsening is at higher churn risk\n",
    "than one who has been consistently neutral or mildly negative.\n",
    "\n",
    "Simple average sentiment scores fail to capture this dynamic behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We compute **sentiment velocity** by:\n",
    "- Ordering each customer’s reviews chronologically\n",
    "- Calculating the change in sentiment score between consecutive orders\n",
    "- Retaining the **most negative sentiment change** per customer\n",
    "\n",
    "The resulting feature captures the **sharpest decline** in customer sentiment,\n",
    "which is highly predictive of churn during crisis periods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d45d6d-6301-4092-a9d4-7b8ad6c2ea7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, min as min_col\n",
    "\n",
    "# Load review scores and timestamps for each customer\n",
    "df_reviews = spark.table(\"food_delivery.silver_orders_enriched\") \\\n",
    "    .select(\"customer_id\", \"created_at_simulated\", \"review_score\")\n",
    "\n",
    "# Define window to order reviews by time for each customer\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"created_at_simulated\")\n",
    "\n",
    "sentiment_velocity = (\n",
    "    df_reviews\n",
    "    # Get previous review score for each order\n",
    "    .withColumn(\n",
    "        \"prev_review_score\",\n",
    "        lag(\"review_score\").over(window_spec)\n",
    "    )\n",
    "    # Calculate change in review score compared to previous order\n",
    "    .withColumn(\n",
    "        \"sentiment_delta\",\n",
    "        col(\"review_score\") - col(\"prev_review_score\")\n",
    "    )\n",
    "    # Aggregate: For each customer, get the minimum sentiment delta (largest drop)\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        min_col(\"sentiment_delta\").alias(\"sentiment_velocity\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3e8b897-0e43-406b-a027-86cbb818c119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 3. RFM Score\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Not all customers carry the same business value.\n",
    "During crisis recovery, intervention resources must be prioritized toward\n",
    "**high-value customers**.\n",
    "\n",
    "Raw order counts or revenue totals alone do not provide a balanced view\n",
    "of customer importance.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We apply **RFM analysis** by scoring customers across three dimensions:\n",
    "\n",
    "- **Recency (R):** How recently the customer ordered\n",
    "- **Frequency (F):** How often the customer orders\n",
    "- **Monetary (M):** How much the customer has spent\n",
    "\n",
    "Each dimension is bucketed into quintiles and combined into a\n",
    "single composite `rfm_score`, producing a compact and interpretable\n",
    "measure of customer lifetime value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9c774e0-d58d-4ef5-b20c-84c10931678c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as sum_col\n",
    "\n",
    "# Aggregate monetary value (total spend) per customer from enriched orders\n",
    "monetary_value = (\n",
    "    spark.table(\"food_delivery.silver_orders_enriched\")\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        sum_col(\"subtotal\").alias(\"monetary_value\")\n",
    "    )\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import ntile\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Join customer features with monetary value\n",
    "rfm_base = (\n",
    "    training_data_indexed   \n",
    "    .join(monetary_value, \"customer_id\", \"left\")\n",
    ")\n",
    "\n",
    "# Define windows for RFM scoring\n",
    "r_window = Window.partitionBy().orderBy(col(\"recency_days\").asc())      # Recency: lower is better\n",
    "f_window = Window.partitionBy().orderBy(col(\"total_orders\").desc())     # Frequency: higher is better\n",
    "m_window = Window.partitionBy().orderBy(col(\"monetary_value\").desc())   # Monetary: higher is better\n",
    "\n",
    "# Calculate RFM scores using quintiles and combine into a single score\n",
    "rfm_scored = (\n",
    "    rfm_base\n",
    "    .withColumn(\"R\", ntile(5).over(r_window))                          # Recency score (1-5)\n",
    "    .withColumn(\"F\", ntile(5).over(f_window))                          # Frequency score (1-5)\n",
    "    .withColumn(\"M\", ntile(5).over(m_window))                          # Monetary score (1-5)\n",
    "    .withColumn(\n",
    "        \"rfm_score\",\n",
    "        col(\"R\") * 100 + col(\"F\") * 10 + col(\"M\")                     # Composite RFM score\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aea43c87-f784-4528-a161-ee4973a8f5f9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"label\":182},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769673218439}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combine all engineered features into a single DataFrame for ML modeling\n",
    "ml_features = (\n",
    "    training_data_indexed         \n",
    "    .join(crisis_flag_per_customer, \"customer_id\", \"left\")         # Add crisis exposure index\n",
    "    .join(sentiment_velocity, \"customer_id\", \"left\")               # Add sentiment velocity feature\n",
    "    .join(rfm_scored.select(\"customer_id\", \"rfm_score\"), \"customer_id\", \"left\")  # Add RFM score\n",
    "    .fillna({\n",
    "        \"crisis_exposure_index\": 0,    # Fill missing crisis exposure with 0\n",
    "        \"sentiment_velocity\": 0,       # Fill missing sentiment velocity with 0\n",
    "        \"rfm_score\": 0                 # Fill missing RFM score with 0\n",
    "    })\n",
    ")\n",
    "\n",
    "# Save features as a Delta table if it doesn't already exist\n",
    "if not spark.catalog.tableExists(\"food_delivery.ml_churn_features\"):\n",
    "    ml_features.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"food_delivery.ml_churn_features\")\n",
    "else:\n",
    "    print(\"ml churn features table already exists → skipping creation\")\n",
    "\n",
    "# Display the final ML features DataFrame\n",
    "display(ml_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd36b50-f8a7-4767-a109-1989441c9bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**Advanced Temporal & Crisis Features** :\n",
    "- crisis_flag_per_customer\n",
    "- sentiment_velocity\n",
    "- rfm_scored\n",
    "\n",
    "These features capture change, shock, or behavioral acceleration so it shos \"how customer sentiment and experience deteriorate over time\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a99724b-10ae-4b92-8322-447d4cf1e3a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display summary statistics (count, mean, stddev, min, max, etc.) for key engineered features\n",
    "display(\n",
    "    ml_features.select(\n",
    "        \"crisis_exposure_index\",\n",
    "        \"sentiment_velocity\",\n",
    "        \"rfm_score\"\n",
    "    ).summary()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc8675ad-0a6e-4cbe-ae5f-88673f9ec1af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Due to extreme class imbalance (0.12% churn rate - derived form mean value,0.9976), supervised churn prediction was not feasible without resampling or label redesign. The analysis instead highlights strong customer retention and the importance of churn definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adbb033b-3fbc-433b-b809-b1e4e62c267f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Downstream Dependencies\n",
    "\n",
    "The ML Feature Engineering layer feeds:\n",
    "\n",
    "- **Churn Prediction Models**\n",
    "  - Baseline churn model (behavioral features only)\n",
    "  - Crisis-aware churn model (temporal + sentiment + RFM features)\n",
    "\n",
    "- **MLflow Experiments**\n",
    "  - Feature set comparisons (v1 vs v2)\n",
    "  - Model versioning and governance\n",
    "  - Recall-driven evaluation for crisis scenarios\n",
    "\n",
    "- **Gold Customer Risk Tables**\n",
    "  - `gold_customer_risk_profile`\n",
    "  - Retention and CRM targeting workflows\n",
    "\n",
    "- **Business Decision Systems**\n",
    "  - Discount / incentive allocation\n",
    "  - Crisis recovery prioritization\n",
    "  - Executive churn risk reporting\n",
    "\n",
    "Any error or leakage in this layer directly impacts\n",
    "**model accuracy, explainability, and business decisions** —\n",
    "which is why feature engineering must be **precise, defensible, and reproducible**.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook establishes a **production-grade feature foundation** for\n",
    "crisis-aware churn prediction by:\n",
    "\n",
    "- Translating raw operational and sentiment data into **model-ready features**\n",
    "- Capturing **temporal dynamics** (recency, velocity, crisis exposure)\n",
    "- Quantifying **customer value** using RFM scoring\n",
    "- Designing features that align with **real business behavior**\n",
    "- Preventing data leakage through customer-level aggregation\n",
    "\n",
    "It forms the **bridge between analytics and AI**, ensuring that\n",
    "machine learning models are trained on signals that are:\n",
    "- Interpretable\n",
    "- Actionable\n",
    "- Aligned with crisis recovery objectives\n",
    "\n",
    "This layer enables models that do not just predict churn,\n",
    "but **explain why customers are at risk**.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Crisis_recovery_ml_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
