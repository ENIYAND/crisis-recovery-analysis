{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0954936d-472f-4e93-af9c-ca4bf2cb3b81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Gold layer\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook builds the **Gold layer** of the Crisis Recovery Lakehouse.\n",
    "\n",
    "The Gold layer represents the **business-facing, decision-ready outputs**\n",
    "used by:\n",
    "- Leadership dashboards\n",
    "- Operations teams\n",
    "- Retention & marketing teams\n",
    "- Crisis response stakeholders\n",
    "\n",
    "Unlike Bronze and Silver, Gold tables are:\n",
    "- Aggregated\n",
    "- Opinionated\n",
    "- Business-aligned\n",
    "- Optimized for reporting and decision-making\n",
    "\n",
    "---\n",
    "\n",
    "## Business Context\n",
    "\n",
    "During a crisis, leadership does not want raw data or row-level facts.\n",
    "\n",
    "They need clear answers to questions like:\n",
    "- Is the crisis getting better or worse?\n",
    "- Which markets and stores are under stress?\n",
    "- Which customers are at risk of churning?\n",
    "- Where should intervention happen first?\n",
    "\n",
    "The Gold layer transforms analytical data into **actionable intelligence**.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs and Outputs\n",
    "\n",
    "### Inputs (from Silver Layer)\n",
    "\n",
    "| Source Table | Purpose |\n",
    "|-------------|---------|\n",
    "| `silver_orders_enriched` | Customer behavior & sentiment |\n",
    "| `silver_sla_metrics` | Operational & delivery performance |\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs (Gold Tables)\n",
    "\n",
    "| Table | Business Purpose |\n",
    "|------|------------------|\n",
    "| `gold_daily_kpis` | Executive-level crisis monitoring |\n",
    "| `gold_customer_churn_risk` | Retention & CRM targeting |\n",
    "| `gold_daily_sla_health` | Operations health tracking |\n",
    "| `gold_store_sla_ranking` | Store-level risk prioritization |\n",
    "| `gold_market_congestion` | Market-level capacity stress |\n",
    "\n",
    "---\n",
    "\n",
    "## Design Principles of the Gold Layer\n",
    "\n",
    "- Tables answer **specific business questions**\n",
    "- Metrics are **pre-computed**, not derived on the fly\n",
    "- Each table serves **one primary audience**\n",
    "- Optimized for fast BI queries\n",
    "- No raw or sensitive text data exposed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7daadde3-4d89-4772-8956-c3093c4c608f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1: Daily Crisis KPIs (`gold_daily_kpis`)\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Executives need a **single daily view** answering:\n",
    "- How severe are delivery delays?\n",
    "- Are customer complaints increasing or decreasing?\n",
    "- Is the crisis stabilizing?\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We aggregate order and sentiment data by day to compute:\n",
    "- Total orders\n",
    "- Average delivery delay\n",
    "- Late delivery complaints\n",
    "- Food safety incidents\n",
    "- Positive feedback trends\n",
    "\n",
    "This table powers the **Crisis Recovery Tracker dashboard**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "822ef71c-ff90-46e6-9ae3-10e2b2fbc627",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769831072246}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count, sum, col, when,to_date\n",
    "\n",
    "# 1. Read from Silver Enriched\n",
    "df_silver = spark.table(\"food_delivery.silver_orders_enriched\")\n",
    "\n",
    "if not spark.catalog.tableExists(\"food_delivery.gold_daily_kpis\"):\n",
    "# 2. Aggregate by Day\n",
    "    gold_daily_kpis = (\n",
    "        df_silver\n",
    "        .withColumn(\"order_date\", to_date(\"created_at_simulated\"))\n",
    "        .groupBy(\"order_date\").agg(\n",
    "        count(\"order_id\").alias(\"total_orders\"),\n",
    "        \n",
    "        # Calculate Average Delivery Time (in minutes)\n",
    "        (avg(col(\"actual_delivery_time_simulated\").cast(\"long\") - col(\"created_at_simulated\").cast(\"long\")) / 60).alias(\"avg_delivery_minutes\"),\n",
    "        \n",
    "        # Count how many reviews were Negative vs Positive\n",
    "        sum(when(col(\"sentiment_category\") == \"Late Delivery\", 1).otherwise(0)).alias(\"late_complaints\"),\n",
    "        sum(when(col(\"sentiment_category\") == \"Food Safety\", 1).otherwise(0)).alias(\"safety_incidents\"),\n",
    "        sum(when(col(\"sentiment_category\") == \"Positive\", 1).otherwise(0)).alias(\"positive_reviews\")\n",
    "    ).orderBy(\"order_date\"))\n",
    "\n",
    "    # 3. Write to Gold Table\n",
    "    gold_daily_kpis.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"food_delivery.gold_daily_kpis\")\n",
    "else:\n",
    "     print(\"Gold Daily KPIs table already exists → skipping creation\")\n",
    "\n",
    "display(spark.table(\"food_delivery.gold_daily_kpis\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f39463aa-f084-4ed7-ac12-8070f1b1138d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2: Customer Churn Risk (`gold_customer_churn_risk`)\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Retention teams must identify:\n",
    "- Customers impacted by the crisis\n",
    "- Customers at risk of leaving\n",
    "- Customers who need proactive offers\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We aggregate customer-level behavior to compute:\n",
    "- Last order date\n",
    "- Lifetime orders\n",
    "- Negative experience frequency\n",
    "- Days since last engagement\n",
    "\n",
    "Based on these signals, we assign **churn risk segments**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Assumption\n",
    "\n",
    "> In a crisis, **recent bad experiences combined with inactivity**\n",
    "> are stronger churn indicators than historical loyalty alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88eb69e5-a2a2-4327-9d05-d96460287b6c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769837623815}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (max as max_date, lit, datediff, current_date, col,\n",
    "    when,\n",
    "    count,\n",
    "    sum,\n",
    "    datediff,to_date)\n",
    "\n",
    "# 1. Read Silver\n",
    "df_silver = spark.table(\"food_delivery.silver_orders_enriched\")\n",
    "\n",
    "if not spark.catalog.tableExists(\"food_delivery.gold_customer_churn_risk\"):\n",
    "\n",
    "    # 2. Customer Level Aggregation\n",
    "    gold_churn = df_silver.groupBy(\"customer_id\", \"customer_name\", \"segment\").agg(\n",
    "        max_date(to_date(\"created_at_simulated\")).alias(\"last_order_date\"),\n",
    "        count(\"order_id\").alias(\"lifetime_orders\"),\n",
    "        \n",
    "        # How many times did they complain about lateness?\n",
    "        sum(when(col(\"sentiment_category\") == \"Late Delivery\", 1).otherwise(0)).alias(\"bad_experiences_count\")\n",
    "    )\n",
    "\n",
    "    # 3. Define Logic: Who is \"At Risk\"?\n",
    "    # Rule: If they had a bad experience AND haven't ordered in the last 45 days (assuming current date is End of 2025)\n",
    "    # Note: We use our simulated \"Current Date\" (2025-12-31) for calculation\n",
    "    simulated_today = to_date(lit(\"2025-12-31\"))\n",
    "\n",
    "    gold_churn_risk = gold_churn.withColumn(\n",
    "        \"days_since_last_order\", \n",
    "        datediff(simulated_today, col(\"last_order_date\"))\n",
    "    ).withColumn(\n",
    "        \"churn_risk_segment\",\n",
    "        when((col(\"bad_experiences_count\") > 0) & (col(\"days_since_last_order\") > 30), \"High Risk - Crisis Victim\")\n",
    "        .when(col(\"days_since_last_order\") > 60, \"High Risk - Natural Churn\")\n",
    "        .otherwise(\"Loyal / Active\")\n",
    "    )\n",
    "\n",
    "    # 4. Write to Gold Table\n",
    "    gold_churn_risk.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"food_delivery.gold_customer_churn_risk\")\n",
    "\n",
    "else:\n",
    "    print(\"Gold Customer Churn Risk table already exists → skipping creation\")\n",
    "\n",
    "# Display the \"High Risk\" customers we need to save\n",
    "display(spark.table(\"food_delivery.gold_customer_churn_risk\").filter(col(\"churn_risk_segment\") == \"High Risk - Crisis Victim\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d5221ef-0fa5-4267-a950-b984544eaf0f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769837807841}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (max as max_date, lit, datediff, current_date, col)\n",
    "\n",
    "display(spark.table(\"food_delivery.gold_customer_churn_risk\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae642f3c-bf98-43d2-b8ff-3ef4cf5ecfc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3: Customer Risk Profile (`gold_customer_risk_profile`)\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "During a crisis, retention teams cannot act on raw order or review data.\n",
    "They need a **single, customer-level snapshot** that answers:\n",
    "\n",
    "- Which customers are likely to churn?\n",
    "- Who requires immediate intervention?\n",
    "- Which dissatisfied customers are most valuable to the business?\n",
    "\n",
    "Event-level data is too granular and slow for operational decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We construct a **denormalized customer risk profile** by aggregating\n",
    "order behavior and sentiment signals:\n",
    "\n",
    "- Recency of last order\n",
    "- Lifetime order frequency\n",
    "- Average order value\n",
    "- Count of negative experiences\n",
    "- Sentiment of the most recent order\n",
    "\n",
    "These signals are combined into:\n",
    "- A **churn probability score** (0–100)\n",
    "- A **lifetime value estimate**\n",
    "- A **last-order sentiment indicator**\n",
    "\n",
    "This table is optimized for direct consumption by\n",
    "CRM systems, dashboards, and retention workflows.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Assumption\n",
    "\n",
    "> In a crisis scenario, **recent inactivity combined with repeated negative experiences**\n",
    "> is a stronger churn signal than historical loyalty alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea7baa2-9a15-403b-82ef-f9feb9260033",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769584728802}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Build Aggregates\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "df_silver = spark.table(\"food_delivery.silver_orders_enriched\")\n",
    "\n",
    "simulated_today = lit(\"2025-12-31\")\n",
    "\n",
    "customer_agg = (\n",
    "    df_silver\n",
    "    .groupBy(\"customer_id\", \"customer_name\", \"segment\")\n",
    "    .agg(\n",
    "        F.to_date(max_date(\"created_at_simulated\")).alias(\"last_order_date\"),\n",
    "        count(\"order_id\").alias(\"lifetime_orders\"),\n",
    "        avg(\"subtotal\").alias(\"avg_order_value\"),\n",
    "        sum(when(col(\"sentiment_category\") != \"Positive\", 1).otherwise(0))\n",
    "            .alias(\"negative_experiences\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "#Lifetime Value\n",
    "customer_agg = customer_agg.withColumn(\n",
    "    \"lifetime_value\",\n",
    "    col(\"lifetime_orders\") * col(\"avg_order_value\")\n",
    ")\n",
    "\n",
    "#Churn Probability Score (0–100)\n",
    "customer_agg = (\n",
    "    customer_agg\n",
    "    .withColumn(\n",
    "        \"days_since_last_order\",\n",
    "        datediff(simulated_today, col(\"last_order_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"churn_probability_score\",\n",
    "        when(col(\"days_since_last_order\") > 90, 80)\n",
    "        .when(col(\"days_since_last_order\") > 60, 60)\n",
    "        .when(col(\"days_since_last_order\") > 30, 40)\n",
    "        .otherwise(20)\n",
    "        + when(col(\"negative_experiences\") > 2, 20).otherwise(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "#Last Order Sentiment\n",
    "last_sentiment = (\n",
    "    df_silver\n",
    "    .withColumn(\n",
    "        \"rn\",\n",
    "        F.row_number().over(\n",
    "            Window.partitionBy(\"customer_id\").orderBy(col(\"created_at_simulated\").desc())\n",
    "        )\n",
    "    )\n",
    "    .filter(col(\"rn\") == 1)\n",
    "    .select(\"customer_id\", col(\"sentiment_category\").alias(\"last_order_sentiment\"))\n",
    ")\n",
    "\n",
    "gold_customer_risk_profile = (\n",
    "    customer_agg\n",
    "    .join(last_sentiment, \"customer_id\", \"left\")\n",
    ")\n",
    "\n",
    "#Write Gold Table\n",
    "gold_customer_risk_profile.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"food_delivery.gold_customer_risk_profile\")\n",
    "\n",
    "display(spark.table(\"food_delivery.gold_customer_risk_profile\").limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3961831b-adfb-4dde-a272-2648ed4cbc31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4: Daily SLA Health (`gold_daily_sla_health`)\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Operations teams need to monitor:\n",
    "- Delivery delays\n",
    "- Capacity utilization\n",
    "- Backlog pressure\n",
    "\n",
    "at a **daily resolution**.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We aggregate SLA metrics by date to calculate:\n",
    "- Average delivery delay\n",
    "- Percentage of delayed orders\n",
    "- Dasher utilization\n",
    "- Outstanding order backlog\n",
    "\n",
    "This enables early detection of operational degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ffe9a36-7834-401c-81a6-d4ac0c3dd938",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Daily SLA Health Table Creation"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count, sum, col, when, to_date\n",
    "\n",
    "# Read Silver SLA metrics table\n",
    "df_sla = spark.table(\"food_delivery.silver_sla_metrics\")\n",
    "\n",
    "# Check if the Gold daily SLA health table already exists to avoid overwriting\n",
    "if not spark.catalog.tableExists(\"food_delivery.gold_daily_sla_health\"):\n",
    "    # Aggregate daily SLA metrics\n",
    "    gold_daily_sla_health = (\n",
    "        df_sla\n",
    "        # Extract order date from simulated creation timestamp\n",
    "        .withColumn(\"order_date\", to_date(\"created_at_simulated\"))\n",
    "        .groupBy(\"order_date\")\n",
    "        .agg(\n",
    "            # Total orders per day\n",
    "            count(\"order_id\").alias(\"total_orders\"),\n",
    "\n",
    "            # Average delivery delay in seconds\n",
    "            avg(\"delivery_delay_seconds\").alias(\"avg_delay_seconds\"),\n",
    "\n",
    "            # Count of delayed orders (delay > 0)\n",
    "            sum(when(col(\"delivery_delay_seconds\") > 0, 1).otherwise(0))\n",
    "                .alias(\"delayed_orders\"),\n",
    "\n",
    "            # Percentage of delayed orders\n",
    "            (sum(when(col(\"delivery_delay_seconds\") > 0, 1).otherwise(0))\n",
    "            / count(\"order_id\") * 100).alias(\"delay_percentage\"),\n",
    "\n",
    "            # Average dasher utilization (busy/onshift), skip if onshift is zero\n",
    "            avg(\n",
    "                when(col(\"total_onshift_dashers\") == 0, None)\n",
    "                .otherwise(col(\"total_busy_dashers\") / col(\"total_onshift_dashers\"))\n",
    "            ).alias(\"avg_dasher_utilization\"),\n",
    "\n",
    "            # Average outstanding orders per day\n",
    "            avg(\"total_outstanding_orders\").alias(\"avg_outstanding_orders\")\n",
    "        )\n",
    "        .orderBy(\"order_date\")\n",
    "    )\n",
    "\n",
    "    # Save the aggregated results as a Gold table for downstream reporting\n",
    "    gold_daily_sla_health.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"food_delivery.gold_daily_sla_health\")\n",
    "else:\n",
    "    print(\"Gold Daily SLA Health table already exists → skipping creation\")\n",
    "\n",
    "# Display a sample of daily SLA health metrics\n",
    "\n",
    "display(spark.table(\"food_delivery.gold_daily_sla_health\").limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d3e767f-9c0e-4f7e-ac17-cee46766468f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5: Store SLA Ranking (`gold_store_sla_ranking`)\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Not all stores contribute equally to the crisis.\n",
    "\n",
    "Operations teams must:\n",
    "- Identify worst-performing stores\n",
    "- Prioritize interventions\n",
    "- Allocate resources effectively\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We rank stores based on:\n",
    "- Delay frequency\n",
    "- Average delay duration\n",
    "- Volume of impacted orders\n",
    "\n",
    "This table supports **store-level accountability**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecdbf089-602d-492e-81b0-50ee4110051a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import avg, count, sum, col, when, to_date\n",
    "\n",
    "# Read the Silver SLA metrics table\n",
    "df_sla = spark.table(\"food_delivery.silver_sla_metrics\")\n",
    "\n",
    "# Check if the Gold store SLA ranking table already exists to avoid overwriting\n",
    "if not spark.catalog.tableExists(\"food_delivery.gold_store_sla_ranking\"):\n",
    "    # Aggregate SLA metrics at the store level\n",
    "    gold_store_sla_ranking = (\n",
    "        df_sla\n",
    "        .groupBy(\"store_id\")\n",
    "        .agg(\n",
    "            # Total orders per store\n",
    "            count(\"order_id\").alias(\"total_orders\"),\n",
    "            # Average delivery delay in seconds per store\n",
    "            avg(\"delivery_delay_seconds\").alias(\"avg_delay_seconds\"),\n",
    "            # Count of delayed orders (delay > 0) per store\n",
    "            sum(when(col(\"delivery_delay_seconds\") > 0, 1).otherwise(0))\n",
    "                .alias(\"delayed_orders\")\n",
    "        )\n",
    "        # Calculate delay rate as a percentage\n",
    "        .withColumn(\n",
    "            \"delay_rate\",\n",
    "            col(\"delayed_orders\") / col(\"total_orders\") * 100\n",
    "        )\n",
    "        # Order stores by delay rate descending\n",
    "        .orderBy(col(\"delay_rate\").desc())\n",
    "    )\n",
    "\n",
    "    # Save the aggregated results as a Gold table for downstream reporting\n",
    "    gold_store_sla_ranking.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"food_delivery.gold_store_sla_ranking\")\n",
    "else:\n",
    "    print(\"Gold store SLA ranking table already exists → skipping creation\")\n",
    "\n",
    "# Display a sample of store SLA ranking metrics\n",
    "display(spark.table(\"food_delivery.gold_store_sla_ranking\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4e74696-e3a2-4532-9137-9e4f945c19fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5: Market Congestion Analysis (`gold_market_congestion`)\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "At scale, crises are often driven by **regional capacity imbalances**.\n",
    "\n",
    "Leadership needs to know:\n",
    "- Which markets are overloaded\n",
    "- Where supply is insufficient\n",
    "- Where demand management is required\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We compute congestion metrics per market using:\n",
    "- Outstanding orders\n",
    "- Busy vs on-shift dashers\n",
    "- Derived congestion ratios\n",
    "\n",
    "This table informs **strategic, market-level decisions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec04fd76-621c-458f-99ef-f4eed3109856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count, sum, col, when, to_date\n",
    "\n",
    "# Read the Silver SLA metrics table\n",
    "df_sla = spark.table(\"food_delivery.silver_sla_metrics\")\n",
    "\n",
    "# Check if the Gold market congestion table already exists to avoid overwriting\n",
    "if not spark.catalog.tableExists(\"food_delivery.gold_market_congestion\"):\n",
    "    # Aggregate congestion metrics at the market level\n",
    "    gold_market_congestion = (\n",
    "        df_sla\n",
    "        .groupBy(\"market_id\")\n",
    "        .agg(\n",
    "            avg(\"total_outstanding_orders\").alias(\"avg_backlog\"),      # Average outstanding orders per market\n",
    "            avg(\"total_busy_dashers\").alias(\"avg_busy_dashers\"),       # Average busy dashers per market\n",
    "            avg(\"total_onshift_dashers\").alias(\"avg_onshift_dashers\")  # Average onshift dashers per market\n",
    "        )\n",
    "        # Calculate congestion ratio: backlog divided by onshift dashers\n",
    "        .withColumn(\n",
    "            \"congestion_ratio\",\n",
    "            col(\"avg_backlog\") / col(\"avg_onshift_dashers\")\n",
    "        )\n",
    "        # Order markets by congestion ratio descending\n",
    "        .orderBy(col(\"congestion_ratio\").desc())\n",
    "    )\n",
    "\n",
    "    # Save the aggregated results as a Gold table for downstream reporting\n",
    "    gold_market_congestion.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"food_delivery.gold_market_congestion\")\n",
    "else:\n",
    "    print(\"Gold Market Congestion table already exists → skipping creation\")\n",
    "\n",
    "# Display a sample of market congestion metrics\n",
    "display(spark.table(\"food_delivery.gold_market_congestion\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95eddf46-285b-4988-81c0-f19e9b039433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "858b4dcc-a097-4730-bb6a-35697e7fa1f1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{\"path\":false}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769586089405}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "OPTIMIZE food_delivery.gold_daily_sla_health\n",
    "ZORDER BY (order_date);\n",
    "\n",
    "OPTIMIZE food_delivery.gold_customer_risk_profile\n",
    "ZORDER BY (churn_probability_score);\n",
    "\n",
    "OPTIMIZE food_delivery.gold_customer_risk_profile\n",
    "ZORDER BY (churn_probability_score);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc0a5a21-47d1-411f-9f11-82116c3fe5db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Downstream Usage\n",
    "\n",
    "Gold tables are consumed by:\n",
    "- Databricks Dashboards\n",
    "- BI tools (Power BI / Tableau)\n",
    "- Executive reviews\n",
    "- Crisis response playbooks\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook converts **trusted analytical data**\n",
    "into **business-ready intelligence**.\n",
    "\n",
    "It represents the final step in the Crisis Recovery Lakehouse,\n",
    "where data becomes **decisions**."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8608475490277627,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Crisis_recovery_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
