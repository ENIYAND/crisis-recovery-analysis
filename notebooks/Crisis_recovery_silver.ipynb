{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "760263e8-39af-48f5-9d59-3d287ca98554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Silver Layer - Quality firewall\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook builds the **Silver layer** of the Crisis Recovery Lakehouse.\n",
    "\n",
    "The Silver layer is responsible for converting **raw Bronze data** into\n",
    "**clean, typed, analytics-ready datasets** that can safely be used for:\n",
    "- Dashboards\n",
    "- Machine Learning\n",
    "- AI enrichment\n",
    "- Business decisioning\n",
    "\n",
    "Unlike the Bronze layer, Silver **enforces structure and quality**.\n",
    "\n",
    "---\n",
    "\n",
    "## Business Context\n",
    "\n",
    "During a crisis, raw operational data contains:\n",
    "- Incorrect timestamps\n",
    "- Invalid delivery durations\n",
    "- Negative or zero item counts\n",
    "- Schema inconsistencies from upstream systems\n",
    "\n",
    "If such data flows directly into analytics or ML pipelines:\n",
    "- KPIs become unreliable\n",
    "- Models learn incorrect patterns\n",
    "- Alerts generate false positives\n",
    "\n",
    "The Silver layer acts as a **quality firewall** between ingestion and insight.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs and Outputs\n",
    "\n",
    "### Input\n",
    "\n",
    "| Source | Description |\n",
    "|------|-------------|\n",
    "| `food_delivery.bronze_orders` | Raw, schema-evolving order events |\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| Table | Purpose |\n",
    "|------|--------|\n",
    "| `silver_orders_clean` | Typed & validated order records |\n",
    "| `silver_orders_enriched` | Customer + review enriched orders |\n",
    "| `silver_sla_metrics` | Operational & SLA-focused metrics |\n",
    "\n",
    "---\n",
    "\n",
    "## Design Principles of the Silver Layer\n",
    "\n",
    "- Enforce **correct data types**\n",
    "- Apply **business validity rules**\n",
    "- Separate datasets by **analytical purpose**\n",
    "- Avoid overloading a single “god table”\n",
    "- Prepare data for **Gold and ML layers**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc98a03e-532c-4e9a-ac0f-ecd430f8c9e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1: Silver Orders – Canonical Cleaning (`silver_orders_clean`)\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Bronze data preserves reality but:\n",
    "- Uses string timestamps\n",
    "- Allows invalid business values\n",
    "- Cannot support event-time analytics\n",
    "\n",
    "Downstream systems require **typed and valid data**.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We create a **canonical Silver table** that:\n",
    "- Casts all columns to correct types\n",
    "- Removes logically invalid records\n",
    "- Preserves all usable order facts\n",
    "\n",
    "This table becomes the **single source of truth** for all downstream Silver and Gold datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Validations Applied\n",
    "\n",
    "- `actual_delivery_time > created_at`\n",
    "- `total_items > 0`\n",
    "- Numeric casting for all metrics\n",
    "- Timestamp conversion for event-time processing\n",
    "\n",
    "---\n",
    "\n",
    "### Why a Canonical Clean Table?\n",
    "\n",
    "Instead of repeating casting logic everywhere:\n",
    "- We clean once\n",
    "- Reuse everywhere\n",
    "- Reduce risk of inconsistent logic\n",
    "\n",
    "This mirrors real production data platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74ff605c-77ff-4a7e-af95-c3334ba756c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, expr, to_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"order_id\").orderBy(col(\"created_at_simulated\").desc())\n",
    "\n",
    "# 2. Apply Quality Constraints \n",
    "# Constraint 1: Delivery must happen AFTER creation\n",
    "# Constraint 2: Order must have items\n",
    "\n",
    "if not spark.catalog.tableExists(\"food_delivery.silver_orders_clean\"):\n",
    "    silver_orders_clean = (\n",
    "    spark.table(\"food_delivery.bronze_orders\")\n",
    "        # ---------- timestamps ----------\n",
    "        .withColumn(\"created_at_simulated\", to_timestamp(\"created_at_simulated\"))\n",
    "        .withColumn(\"actual_delivery_time_simulated\", to_timestamp(\"actual_delivery_time_simulated\"))\n",
    "\n",
    "        # ---------- numeric sanity ----------\n",
    "        .withColumn(\"total_items\", col(\"total_items\").cast(\"int\"))\n",
    "        .withColumn(\"subtotal\", col(\"subtotal\").cast(\"int\"))\n",
    "        .withColumn(\"num_distinct_items\", col(\"num_distinct_items\").cast(\"int\"))\n",
    "        .withColumn(\"min_item_price\", col(\"min_item_price\").cast(\"int\"))\n",
    "        .withColumn(\"max_item_price\", col(\"max_item_price\").cast(\"int\"))\n",
    "\n",
    "        .withColumn(\"estimated_order_place_duration\", col(\"estimated_order_place_duration\").cast(\"int\"))\n",
    "        .withColumn(\"estimated_store_to_consumer_driving_duration\",\n",
    "                    col(\"estimated_store_to_consumer_driving_duration\").cast(\"int\"))\n",
    "\n",
    "        .withColumn(\"total_busy_dashers\", col(\"total_busy_dashers\").cast(\"int\"))\n",
    "        .withColumn(\"total_onshift_dashers\", col(\"total_onshift_dashers\").cast(\"int\"))\n",
    "        .withColumn(\"total_outstanding_orders\", col(\"total_outstanding_orders\").cast(\"int\"))\n",
    "\n",
    "        # ---------- IDs ----------\n",
    "        .withColumn(\"order_id\", col(\"order_id\").cast(\"long\"))\n",
    "        .withColumn(\"customer_id\", col(\"customer_id\").cast(\"long\"))\n",
    "        .withColumn(\"store_id\", col(\"store_id\").cast(\"int\"))\n",
    "        .withColumn(\"market_id\", col(\"market_id\").cast(\"int\"))\n",
    "\n",
    "        # ------------Null protection on critical columns -----------\n",
    "        .filter(\n",
    "            col(\"created_at_simulated\").isNotNull() &\n",
    "            col(\"actual_delivery_time_simulated\").isNotNull() &\n",
    "            col(\"order_id\").isNotNull() &\n",
    "            col(\"customer_id\").isNotNull()\n",
    "        )\n",
    "\n",
    "        # ---------- Deduplication -----------\n",
    "        .withColumn(\"rn\", row_number().over(window_spec))\n",
    "        .filter(col(\"rn\") == 1)\n",
    "        .drop(\"rn\")\n",
    "\n",
    "        # ---------- data quality filters ----------\n",
    "        .filter(\n",
    "            (col(\"actual_delivery_time_simulated\") > col(\"created_at_simulated\")) &\n",
    "            (col(\"total_items\") > 0) &  (col(\"subtotal\") >= 0) &  (col(\"min_item_price\") >= 0) & (col(\"max_item_price\") >= col(\"min_item_price\"))\n",
    "        )\n",
    "\n",
    "     \n",
    "    )\n",
    "        # ---------- silver_orders_clean table is created ----------\n",
    "    silver_orders_clean.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"food_delivery.silver_orders_clean\")\n",
    "\n",
    "    print(\"Silver clean table created.\")\n",
    "\n",
    "else:\n",
    "    print(\"Silver clean table already exists → skipping creation\")\n",
    "# Print the schema tree to the console\n",
    "spark.table(\"food_delivery.silver_orders_clean\").printSchema()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1556e49e-44b2-4f5a-8857-7cc9ee4bc668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2: Silver Customer Enrichment (`silver_orders_enriched`)\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Customer experience analysis requires:\n",
    "- Order behavior\n",
    "- Customer attributes\n",
    "- Review sentiment signals\n",
    "\n",
    "These datasets exist separately and must be joined carefully.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We enrich the clean Silver orders by:\n",
    "- Joining with `dim_customers`\n",
    "- Joining with `fact_reviews`\n",
    "- Producing a **Customer 360 view**\n",
    "\n",
    "This table answers:\n",
    "> “What happened, to whom, and how did they feel?”\n",
    "\n",
    "---\n",
    "\n",
    "### Streaming Design Note\n",
    "\n",
    "Although joins are **Stream → Static**, we:\n",
    "- Apply watermarks\n",
    "- Preserve event-time semantics\n",
    "\n",
    "This makes the pipeline **future-ready** for windowed analytics and alerts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faaac16f-7822-4b01-bb59-c1ed21f23d65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "if not spark.catalog.tableExists(\"food_delivery.silver_orders_enriched\"):\n",
    "    silver_enriched = (\n",
    "        spark.table(\"food_delivery.silver_orders_clean\").alias(\"orders\")\n",
    "        # --------------------------------------------------\n",
    "        # Enrich with Customer Dimension (Lookup Join)\n",
    "        # --------------------------------------------------\n",
    "        .join(\n",
    "            spark.table(\"food_delivery.dim_customers\").alias(\"customers\"),\n",
    "            on=\"customer_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "        # --------------------------------------------------\n",
    "        # Enrich with Reviews Fact (Lookup Join)\n",
    "        # --------------------------------------------------\n",
    "        .join(\n",
    "            spark.table(\"food_delivery.fact_reviews\").alias(\"reviews\"),\n",
    "            on=\"order_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "        # --------------------------------------------------\n",
    "        # Final Enriched Schema\n",
    "        # --------------------------------------------------\n",
    "        .select(\n",
    "            \"orders.order_id\",\n",
    "            \"orders.created_at_simulated\",\n",
    "            \"orders.actual_delivery_time_simulated\",\n",
    "            \"orders.store_id\",\n",
    "            \"orders.subtotal\",\n",
    "            \"customers.customer_id\",\n",
    "            \"customers.customer_name\",\n",
    "            \"customers.segment\",\n",
    "            \"reviews.review_score\",\n",
    "            \"reviews.sentiment_category\",\n",
    "            \"reviews.review_text\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ---------- silver_enriched table is created ----------\n",
    "    silver_enriched.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"food_delivery.silver_orders_enriched\")\n",
    "\n",
    "    print(\"Silver Customer Enriched table built (batch).\")\n",
    "\n",
    "else:\n",
    "    print(\"Silver clean table already exists → skipping creation\")\n",
    "\n",
    "# Print the schema tree to the console\n",
    "spark.table(\"food_delivery.silver_orders_clean\").printSchema()\n",
    "\n",
    "spark.table(\"food_delivery.silver_orders_enriched\").show(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91401d0f-2d63-433f-955d-8b79157f5a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3: SLA & Operational Metrics (`silver_sla_metrics`)\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "Operational teams care about:\n",
    "- Delivery delays\n",
    "- Dasher load\n",
    "- Order backlog pressure\n",
    "\n",
    "These signals are **different** from customer analytics and should not be mixed.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach\n",
    "\n",
    "We derive a dedicated SLA table that:\n",
    "- Calculates delivery delays\n",
    "- Retains operational load indicators\n",
    "- Focuses on store & market performance\n",
    "\n",
    "This separation enables:\n",
    "- Faster queries\n",
    "- Clear ownership\n",
    "- Focused Gold aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2469046-8d6a-4de0-8701-a34e91a6f3c3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SLA Metrics Table Creation"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Check if the SLA metrics table already exists to avoid overwriting\n",
    "if not spark.catalog.tableExists(\"food_delivery.silver_sla_metrics\"):\n",
    "    # Read cleaned order data from Silver layer\n",
    "    silver_sla_metrics = (\n",
    "        spark.table(\"food_delivery.silver_orders_clean\")\n",
    "        # Calculate delivery delay in seconds: actual delivery time minus order creation and estimated driving duration\n",
    "        .withColumn(\n",
    "            \"delivery_delay_seconds\",\n",
    "            col(\"actual_delivery_time_simulated\").cast(\"long\")\n",
    "            - col(\"created_at_simulated\").cast(\"long\")\n",
    "            - col(\"estimated_store_to_consumer_driving_duration\")\n",
    "        )\n",
    "        # Select operational and SLA-relevant columns for focused analysis\n",
    "        .select(\n",
    "            \"order_id\",\n",
    "            \"store_id\",\n",
    "            \"market_id\",\n",
    "            \"created_at_simulated\",\n",
    "            \"actual_delivery_time_simulated\",\n",
    "            \"estimated_order_place_duration\",\n",
    "            \"estimated_store_to_consumer_driving_duration\",\n",
    "            \"delivery_delay_seconds\",\n",
    "            \"total_busy_dashers\",\n",
    "            \"total_onshift_dashers\",\n",
    "            \"total_outstanding_orders\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save the resulting DataFrame as a Delta table for reliable downstream use\n",
    "    silver_sla_metrics.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"food_delivery.silver_sla_metrics\")\n",
    "\n",
    "    print(\"Silver SLA metrics table created.\")\n",
    "else:\n",
    "    print(\"Silver SLA metrics table already exists → skipping creation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d8185f4-6a82-41b4-92e8-fe6a79699a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"food_delivery.silver_sla_metrics\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8a07a89-1c2f-46cd-bc0e-946b387bfd47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Downstream Dependencies\n",
    "\n",
    "Silver outputs feed:\n",
    "- Gold KPI dashboards\n",
    "- Churn prediction models\n",
    "- AI sentiment analysis\n",
    "- Geospatial intelligence pipelines\n",
    "\n",
    "Any error here directly impacts business decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook transforms **raw ingestion data** into\n",
    "**trusted analytical assets** by enforcing structure, quality, and purpose-driven design.\n",
    "\n",
    "It is the **most critical quality gate** in the Crisis Recovery Lakehouse."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4551021222152077,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Crisis_recovery_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
